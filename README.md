# Learning GPT
 - Generative Pre-trained Transformer.
 - A language model trained with both supervised and reinforcement learning. 
 
# Key Concept
## tokeniser
 - tiktoken: openAI [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser: https://github.com/openai/tiktoken
 - sentencepiece: google's unsupervised text tokenizer and detokenizer : https://github.com/google/sentencepiece
 - string <-> int
   - string could be char, word, sub-word.
   - int: [0, vocabulary]
   - example: openAI gpt2: https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json


# References
- nanoGPT: https://github.com/karpathy/nanoGPT
